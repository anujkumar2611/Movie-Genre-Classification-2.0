{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-12-02T22:44:56.348454Z","iopub.status.busy":"2022-12-02T22:44:56.348139Z","iopub.status.idle":"2022-12-02T22:44:56.36895Z","shell.execute_reply":"2022-12-02T22:44:56.367901Z","shell.execute_reply.started":"2022-12-02T22:44:56.34843Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Movie Genre Classification With Machine Learning**\n","\n","Using the dataset found [here](https://www.kaggle.com/datasets/hijest/genre-classification-dataset-imdb) we will build a machine learning model which will predict the genre of a movie based off the description provided. "]},{"cell_type":"markdown","metadata":{},"source":["# **Import Dependencies**\n","\n","First things first, we must import the dependencies required to build the model. \n","\n","Here is a quick summary of what each dependency is used for: \n","*      **random**: shuffle the features in the data\n","*      **numpy**: create np.array\n","*      **nltk**: tokenization & lemmatization of sentences & words\n","*      **tensorflow**: model creation\n","    "]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T22:44:56.371062Z","iopub.status.busy":"2022-12-02T22:44:56.370379Z","iopub.status.idle":"2022-12-02T22:45:06.946688Z","shell.execute_reply":"2022-12-02T22:45:06.944864Z","shell.execute_reply.started":"2022-12-02T22:44:56.371034Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\verma\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["import random\n","import numpy as np\n","\n","import nltk\n","nltk.download('omw-1.4')\n","from nltk.stem import WordNetLemmatizer\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout\n","from tensorflow.keras.optimizers import SGD"]},{"cell_type":"markdown","metadata":{},"source":["# **EDA**\n","    \n","Our target variable is \"Genre\", leaving \"ID\", \"Title\" and \"Description\" as our feature variables. \n","\n","ID is just the index of the movie in our dataset, meaning that it has no significance to the target variable\n","Title is the name of the movie, it is unlikely that it has enough information to determine the genre, so it is ruled out. \n","\n","By process of elimination the Description of the movie is determined to be the only significant feature in our dataset. "]},{"cell_type":"markdown","metadata":{},"source":["# **Data Processing**\n","\n","**Reading In Data**\n","\n","First we open the datafile and read it line by line. Due to the nature of the text file each line is a single datapoint representing a movie. Splitting the line with \" ::: \" seperates the variables into an array with the following format: \n","\n","    [ID, Title, Genre, Description]\n","\n","We then extract the genre and description into seperate arrays. \n","\n","**Note only the first thousand datapoints are processed and later used to develop the model, this is done to save time when building the model.*"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T22:45:06.950354Z","iopub.status.busy":"2022-12-02T22:45:06.948945Z","iopub.status.idle":"2022-12-02T22:45:07.364842Z","shell.execute_reply":"2022-12-02T22:45:07.362751Z","shell.execute_reply.started":"2022-12-02T22:45:06.950315Z"},"trusted":true},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()\n","TOTAL_DATA_POINTS = 1000\n","genres = []\n","descriptions = []\n","\n","with open('train_data.txt', encoding='utf-8') as f:\n","    # Your processing code here\n","    counter = 0\n","    for line in f:\n","        if counter <= TOTAL_DATA_POINTS:\n","            data = line.split(\" ::: \")\n","            genres.append(data[2])\n","            descriptions.append(data[3])\n","            counter += 1\n","            "]},{"cell_type":"markdown","metadata":{},"source":["**Tokenization & Lemmatization**\n","\n","Next the descriptions are tokenized and lemmatized and added to words[] which represents the vocabulary of our model. \n","\n","The same thing is done to the genres, and they are added to classes[]\n","\n","The tokenized array of each description & its corresponding genre are added to documents[] as a tuple. Here is an example of the format of document: \n","\n","    [(['This', 'movie', 'is', 'funny'], 'comedy'), (['This', 'movie', 'is', 'scary'], 'horror')]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T22:45:07.368696Z","iopub.status.busy":"2022-12-02T22:45:07.368179Z","iopub.status.idle":"2022-12-02T22:45:10.388025Z","shell.execute_reply":"2022-12-02T22:45:10.387105Z","shell.execute_reply.started":"2022-12-02T22:45:07.368649Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(['Listening', 'in', 'to', 'a', 'conversation', 'between', 'his', 'doctor', 'and', 'parents', ',', '10-year-old', 'Oscar', 'learns', 'what', 'nobody', 'has', 'the', 'courage', 'to', 'tell', 'him', '.', 'He', 'only', 'has', 'a', 'few', 'weeks', 'to', 'live', '.', 'Furious', ',', 'he', 'refuses', 'to', 'speak', 'to', 'anyone', 'except', 'straight-talking', 'Rose', ',', 'the', 'lady', 'in', 'pink', 'he', 'meets', 'on', 'the', 'hospital', 'stairs', '.', 'As', 'Christmas', 'approaches', ',', 'Rose', 'uses', 'her', 'fantastical', 'experiences', 'as', 'a', 'professional', 'wrestler', ',', 'her', 'imagination', ',', 'wit', 'and', 'charm', 'to', 'allow', 'Oscar', 'to', 'live', 'life', 'and', 'love', 'to', 'the', 'full', ',', 'in', 'the', 'company', 'of', 'his', 'friends', 'Pop', 'Corn', ',', 'Einstein', ',', 'Bacon', 'and', 'childhood', 'sweetheart', 'Peggy', 'Blue', '.'], 'drama')\n"]}],"source":["words = []\n","classes = []\n","documents = []\n","ignore_letters = ['?', '!', '.', ',']\n","\n","for i in range(TOTAL_DATA_POINTS):\n","    word_list = nltk.word_tokenize(descriptions[i])\n","    words.extend(word_list)\n","    documents.append((word_list, genres[i]))\n","    if genres[i] not in classes:\n","        classes.append(genres[i])\n","\n","# lemmatize words & add them to words[] if they're not in ignore_letters\n","words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n","words = sorted(set(words))          # remove any duplicates\n","\n","print(documents[0])"]},{"cell_type":"markdown","metadata":{},"source":["**Numerically Representing The Data**\n","\n","We must then numerically represent our data in order to be able to use it to train our model. To do this we will convert documents[] into a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model). Bag of words works by representing a word as a 1 at the index it is found at in its parent (string) array. Building on the previous example: \n","\n","    documents = [(['This', 'movie', 'is', 'funny'], 'comedy'), (['This', 'movie', 'is', 'scary'], 'horror')]\n","    classes = ['comedy', 'horror']\n","    words = ['This', 'movie', 'is', 'funny', 'scary']\n","    \n","    training = [[[1, 1, 1, 1, 0], [1, 0]], [[1, 1, 1, 0, 1], [0, 1]]]   <- Bag of Words representation of documents"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T22:45:10.390002Z","iopub.status.busy":"2022-12-02T22:45:10.389206Z","iopub.status.idle":"2022-12-02T22:45:43.166731Z","shell.execute_reply":"2022-12-02T22:45:43.165308Z","shell.execute_reply.started":"2022-12-02T22:45:10.389967Z"},"trusted":true},"outputs":[],"source":["# create training data\n","training = []\n","# create empty array for our output\n","output_empty = [0] * len(classes)\n","\n","# training set, bag of words for each sentence\n","for document in documents:\n","    # initialize bag of words\n","    bag = []\n","    # list of tokenized words for the pattern\n","    word_patterns = document[0]\n","    # lemmatize each word in the pattern\n","    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n","    # create our bag of words array with 1 if word found in current pattern\n","    for word in words:\n","        bag.append(1) if word in word_patterns else bag.append(0)\n","\n","    # output is 0 for each tag and 1 for current tag (for each pattern)\n","    output_row = list(output_empty)\n","    output_row[classes.index(document[1])] = 1\n","    training.append([bag, output_row])"]},{"cell_type":"markdown","metadata":{},"source":["**Shuffling and Seperating Data**\n","\n","Lastly, the order of the data is shuffled, and it is then seperated into target and feature variables. "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T22:45:43.169007Z","iopub.status.busy":"2022-12-02T22:45:43.16825Z","iopub.status.idle":"2022-12-02T22:45:43.177814Z","shell.execute_reply":"2022-12-02T22:45:43.176786Z","shell.execute_reply.started":"2022-12-02T22:45:43.168979Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\verma\\AppData\\Local\\Temp\\ipykernel_15668\\3612126039.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  training = np.array(training)\n"]}],"source":["# shuffle features and turn into np.array\n","random.shuffle(training)\n","training = np.array(training)\n","\n","# create train lists. X - patterns, Y - intents\n","train_x = list(training[:, 0])\n","train_y = list(training[:, 1])"]},{"cell_type":"markdown","metadata":{},"source":["# Model Development"]},{"cell_type":"markdown","metadata":{},"source":["A sequential model is built with 3 layers. The first layer has as many neurons as words in our vocabulary, the second layer has half as many neurons, and the final output layer has as many neuorns as genres. \n","\n","The loss is calculated with 'categorical_crossentropy' since there are multiple target categories. \n","\n","The model is then saved to be used here."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-02T22:45:43.179491Z","iopub.status.busy":"2022-12-02T22:45:43.179243Z","iopub.status.idle":"2022-12-02T22:59:43.059909Z","shell.execute_reply":"2022-12-02T22:59:43.059165Z","shell.execute_reply.started":"2022-12-02T22:45:43.179468Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/11\n","200/200 [==============================] - 794s 4s/step - loss: 2.4438 - accuracy: 0.3250\n","Epoch 2/11\n","200/200 [==============================] - 586s 3s/step - loss: 1.8740 - accuracy: 0.5020\n","Epoch 3/11\n","200/200 [==============================] - 523s 3s/step - loss: 1.3219 - accuracy: 0.6510\n","Epoch 4/11\n","200/200 [==============================] - 506s 3s/step - loss: 0.8597 - accuracy: 0.7690\n","Epoch 5/11\n","200/200 [==============================] - 547s 3s/step - loss: 0.5862 - accuracy: 0.8420\n","Epoch 6/11\n","200/200 [==============================] - 567s 3s/step - loss: 0.5558 - accuracy: 0.8600\n","Epoch 7/11\n","200/200 [==============================] - 498s 2s/step - loss: 0.3019 - accuracy: 0.9220\n","Epoch 8/11\n","200/200 [==============================] - 471s 2s/step - loss: 0.4086 - accuracy: 0.9110\n","Epoch 9/11\n","200/200 [==============================] - 515s 3s/step - loss: 0.8563 - accuracy: 0.8390\n","Epoch 10/11\n","200/200 [==============================] - 564s 3s/step - loss: 0.7727 - accuracy: 0.8730\n","Epoch 11/11\n","200/200 [==============================] - 521s 3s/step - loss: 0.4624 - accuracy: 0.9050\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\verma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["from tensorflow.keras.optimizers.legacy import SGD\n","# create train lists. X - patterns, Y - intents\n","train_x = list(training[:, 0])\n","train_y = list(training[:, 1])\n","\n","num_words = len(words)\n","num_class = len(classes)\n","\n","\n","# create model - 3 layers. First layer contains as many neurons as words, second layer contains half as many neurons as the \n","# first layer and 3rd output layer contains as many neurons as classes\n","model = Sequential()\n","model.add(Dense(num_words, input_shape=(len(train_x[0]),), activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense((num_words/2), activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_class, activation='softmax'))\n","\n","# Compile model.\n","sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","\n","# Fitting and saving the model.\n","hist = model.fit(np.array(train_x), np.array(train_y), epochs=11, batch_size=5, verbose=1)\n","model.save('GenreClassification.h5', hist)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
